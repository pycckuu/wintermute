# PFAR Feature Spec: Self-Extending Skills

> **Feature**: Agent creates, tests, deploys, and iteratively improves its own tools through conversation  
> **Status**: Design  
> **Priority**: Phase 4  
> **Depends on**: Dynamic Integrations (Phase 3), Pipeline (Phase 2), Vault (Phase 1)

---

## 1. Problem

PFAR can connect pre-built MCP servers (Notion, GitHub, Slack) via the Dynamic Integrations spec. But when the owner asks for something no MCP server exists for ‚Äî "check my server uptime at status.myapp.io", "parse my bank CSV into categories", "post to my Telegram channel at 9am with a weather summary" ‚Äî the agent can't help. The owner must either find an MCP server, write code manually, or go without.

OpenClaw solves this by letting the agent write SKILL.md files with arbitrary code. But OpenClaw skills run with full system access ‚Äî no sandboxing, no credential isolation, no data flow control. The ClawHavoc incident (341 malicious skills, 11.9% infection rate on ClawHub) shows what happens without security boundaries.

PFAR needs the same self-extension capability with the security model already in place: bubblewrap sandboxes, vault-scoped credentials, label-based data flow, and owner approval for destructive operations.

---

## 2. Core Insight

**A self-created skill is just an MCP server that PFAR writes instead of the owner installing.**

The Dynamic Integrations spec already handles:
- Spawning MCP servers in bubblewrap sandboxes
- Network isolation via domain proxy
- Credential injection via vault
- Tool discovery via `tools/list`
- Verification via test API call
- Registration in the tool registry

Self-extension adds one thing: **the LLM generates the MCP server code**. Everything downstream ‚Äî sandboxing, discovery, registration, calling ‚Äî is identical.

```
Third-party integration:     Owner installs ‚Üí Kernel spawns ‚Üí Tools available
Self-created skill:          LLM generates ‚Üí Kernel tests ‚Üí Kernel spawns ‚Üí Tools available
                                    ‚Üë                              ‚Üë
                             Same sandbox.                   Same registry.
                             Same vault.                     Same pipeline.
                             Same labels.                    Same proxy.
```

---

## 3. Skill Format

A skill is a directory in `~/.pfar/skills/`:

```
~/.pfar/skills/
  uptime-checker/
    skill.toml          # manifest (like mcp/*.toml but auto-generated)
    server.py           # FastMCP server (generated by LLM)
    test_server.py      # test cases (generated by LLM)
```

### skill.toml

```toml
# Auto-generated by PFAR SkillForge
name = "uptime-checker"
description = "Check HTTP endpoint uptime, response time, and SSL certificate expiry"
version = "1"
created_at = "2026-02-15T12:00:00Z"
created_by = "agent"            # "agent" or "owner"
label = "public"                # security label for outputs
allowed_domains = ["status.myapp.io", "api.myapp.io"]

[server]
command = "python3"
args = ["server.py"]
working_dir = "~/.pfar/skills/uptime-checker"

[auth]
# empty ‚Äî no credentials needed
# or: MY_API_KEY = "vault:myapp_api_key"

[sandbox]
memory_limit = "128m"
read_only_fs = true
allow_tmp = true

# Metadata for retrieval
[search]
keywords = ["uptime", "status", "health check", "ping", "server monitoring", "SSL", "certificate"]
example_queries = [
    "is my server up?",
    "check uptime for status.myapp.io",
    "when does my SSL cert expire?",
]
```

### server.py (FastMCP)

```python
#!/usr/bin/env python3
"""PFAR Skill: uptime-checker ‚Äî generated by SkillForge v1"""
from mcp.server.fastmcp import FastMCP
import httpx
import ssl
import socket
from datetime import datetime

mcp = FastMCP("uptime-checker")

@mcp.tool()
async def check_uptime(url: str) -> dict:
    """Check if an HTTP endpoint is up. Returns status code, response time, and availability."""
    start = datetime.now()
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            resp = await client.get(url)
            elapsed = (datetime.now() - start).total_seconds()
            return {
                "status": "up",
                "status_code": resp.status_code,
                "response_time_seconds": round(elapsed, 3),
                "url": url,
            }
    except Exception as e:
        return {"status": "down", "error": str(e), "url": url}

@mcp.tool()
async def check_ssl_expiry(hostname: str, port: int = 443) -> dict:
    """Check SSL certificate expiry date for a hostname."""
    try:
        ctx = ssl.create_default_context()
        with ctx.wrap_socket(socket.socket(), server_hostname=hostname) as s:
            s.settimeout(5)
            s.connect((hostname, port))
            cert = s.getpeercert()
            expiry = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
            days_left = (expiry - datetime.utcnow()).days
            return {
                "hostname": hostname,
                "expires": cert['notAfter'],
                "days_remaining": days_left,
                "expired": days_left < 0,
            }
    except Exception as e:
        return {"error": str(e), "hostname": hostname}

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

This is a **real MCP server**. The kernel spawns it in bubblewrap exactly like Notion or GitHub. Same sandbox, same proxy, same tool discovery. The only difference is that the LLM wrote it.

---

## 4. SkillForge: The Creation Pipeline

SkillForge is a KernelFlow (same state machine pattern as integration setup) that manages skill creation.

### States

```rust
pub enum SkillForgeState {
    /// Gathering requirements from conversation
    Designing {
        requirements: Vec<String>,
        api_docs: Option<String>,
    },

    /// LLM is generating code
    Generating,

    /// Running tests in sandbox
    Testing {
        attempt: u8,        // max 3
    },

    /// Waiting for owner to approve (if skill needs credentials or network)
    AwaitingApproval {
        skill_summary: String,
        requested_permissions: Permissions,
    },

    /// Acquiring credentials (delegates to credential flow)
    AcquiringCredentials,

    /// Deploying as MCP server
    Deploying,

    /// Skill is live
    Complete,

    /// Generation or testing failed after retries
    Failed { error: String },
}
```

### The flow

```
"I need to check uptime for my servers"
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Designing   ‚îÇ ‚Üê Gather requirements. Ask clarifying questions.
    ‚îÇ              ‚îÇ   "Which URLs? Need auth? How often?"
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ requirements clear
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Generating   ‚îÇ ‚Üê LLM writes server.py + skill.toml + test_server.py
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     fail (max 3)
    ‚îÇ  Testing     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              ‚îÇ               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚ñº
           ‚îÇ pass          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ               ‚îÇ Generating   ‚îÇ ‚Üê LLM sees error, regenerates
           ‚ñº               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Approval    ‚îÇ ‚Üê Only if: network access, credentials, or write semantics
    ‚îÇ  (optional)  ‚îÇ   Auto-approved if: no network, no creds, read-only
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ approved
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Deploying   ‚îÇ ‚Üê Spawn in bubblewrap, MCP handshake, register tools
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Complete    ‚îÇ ‚Üí "‚úì uptime-checker ready. 2 tools: check_uptime, check_ssl_expiry"
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Trigger: How skill creation starts

Two paths:

**Explicit**: Owner says "create a tool that...", "build a skill for...", "make me a script that..."

```rust
fn parse_skill_request(text: &str) -> Option<SkillRequest> {
    let lower = text.to_lowercase();
    let patterns = [
        "create a tool", "create a skill", "build a tool", "build a skill",
        "make a tool", "make a script", "write a tool", "write a script",
        "i need a tool", "can you make", "build me a",
    ];
    // ...
}
```

**Implicit** (Phase 4+): Planner recognizes no tool matches, suggests creation.

```
Phase 1 (Plan):
  intent: "check uptime for status.myapp.io"
  matching_tools: []  ‚Üê no match
  plan: {
    suggestion: "no_matching_tool",
    proposed_skill: "HTTP uptime checker with response time measurement",
    needs_confirmation: true
  }

Phase 3 (Synthesize):
  "I don't have a tool for checking server uptime yet.
   Want me to create one? It would check HTTP status, response time,
   and optionally SSL certificate expiry."

Owner: "yeah do it"
‚Üí SkillForge activates
```

---

## 5. Code Generation

The LLM generates three files in a single call. The prompt includes:

1. The skill requirements (from Designing state)
2. The FastMCP template (fixed boilerplate)
3. The sandbox constraints (what the skill CAN'T do)
4. Examples of well-formed skills (few-shot)

### Generation prompt structure

```
You are generating a sandboxed MCP tool for PFAR.

REQUIREMENTS:
{requirements from conversation}

CONSTRAINTS:
- The skill runs inside a bubblewrap sandbox with no filesystem access except /tmp
- Network access is restricted to declared domains only: {allowed_domains}
- Credentials are injected as environment variables, never hardcoded
- The skill MUST use FastMCP (from mcp.server.fastmcp import FastMCP)
- Available packages: httpx, beautifulsoup4, lxml, pyyaml, toml, pandas, Pillow
- Each tool function must have a docstring (used by the planner for selection)
- Return structured data (dict/list), not formatted strings

TEMPLATE:
{fastmcp_template}

EXAMPLE SKILL:
{example_skill}

Generate three files:
1. server.py ‚Äî the FastMCP server with tool functions
2. skill.toml ‚Äî the manifest with name, description, allowed_domains, search keywords
3. test_server.py ‚Äî pytest tests that validate each tool works correctly

Output as three fenced code blocks labeled ```server.py```, ```skill.toml```, ```test_server.py```.
```

### Why FastMCP

FastMCP is a ~200-line Python library that wraps the MCP protocol. The LLM only needs to write decorated functions ‚Äî no protocol knowledge required. This makes generation reliable. The alternative (generating a full JSON-RPC stdio server) has too many failure modes.

FastMCP is pre-installed in the skill runtime image. The LLM never needs to handle `pip install`.

---

## 6. Testing in Sandbox

Before deployment, the skill runs in a **throwaway bubblewrap sandbox** ‚Äî identical to production but ephemeral.

```rust
impl SkillForge {
    async fn test_skill(&self, skill_dir: &Path) -> Result<TestResult> {
        // 1. Spawn test sandbox (same bwrap config as production)
        let sandbox = self.sandbox_manager.spawn_ephemeral(
            &skill_dir,
            &skill_config.allowed_domains,
            Duration::from_secs(30),  // hard timeout
        )?;

        // 2. Run pytest inside sandbox
        let test_output = sandbox.exec(
            "python3", &["-m", "pytest", "test_server.py", "-v", "--tb=short"]
        ).await?;

        // 3. If tests pass, also start the MCP server and verify handshake
        if test_output.exit_code == 0 {
            let mcp_check = sandbox.exec(
                "python3", &["server.py", "--check"]  // FastMCP health check mode
            ).await?;

            return Ok(TestResult {
                passed: mcp_check.exit_code == 0,
                stdout: test_output.stdout,
                stderr: test_output.stderr,
                tools_discovered: mcp_check.tools,
            });
        }

        Ok(TestResult {
            passed: false,
            stdout: test_output.stdout,
            stderr: test_output.stderr,
            tools_discovered: vec![],
        })
    }
}
```

If tests fail, the error output feeds back to the LLM for regeneration (max 3 attempts). This is the Voyager pattern ‚Äî execute, observe, fix, retry.

```rust
FlowState::Testing { attempt } => {
    let result = self.test_skill(&skill_dir).await?;

    if result.passed {
        // Check if approval needed
        if self.needs_approval(&skill_config) {
            flow.state = SkillForgeState::AwaitingApproval { ... };
        } else {
            flow.state = SkillForgeState::Deploying;
        }
        self.advance(principal).await
    } else if attempt >= 3 {
        flow.state = SkillForgeState::Failed {
            error: format!("Tests failed after 3 attempts: {}", result.stderr),
        };
        self.gateway.send(principal,
            "Skill creation failed after 3 attempts. Here's the last error ‚Äî \
             want me to try a different approach?"
        ).await;
        Ok(())
    } else {
        // Feed error back to LLM, regenerate
        flow.state = SkillForgeState::Generating;
        flow.context.push(format!(
            "Previous attempt failed:\n{}\n{}\nFix the issues and regenerate.",
            result.stdout, result.stderr
        ));
        self.advance(principal).await
    }
}
```

---

## 7. Approval Gates

Not all skills are auto-deployed. Risk-tiered approval:

| Skill type | Example | Auto-approved? |
|---|---|---|
| Pure computation, no network | CSV parser, text formatter, calculator | ‚úÖ Yes |
| Network read-only, public APIs | Weather checker, stock price lookup | ‚úÖ Yes |
| Network to owner's services | Uptime checker for myapp.io | ‚ö†Ô∏è Confirm domains |
| Requires credentials | API wrapper needing auth token | ‚ùå Explicit approval |
| Write operations | Post to API, send messages, modify data | ‚ùå Explicit approval |

```rust
fn needs_approval(&self, config: &SkillConfig) -> bool {
    // Credentials ‚Üí always approve
    if !config.auth.is_empty() { return true; }

    // Write semantics ‚Üí always approve
    if config.tools.iter().any(|t| t.semantics == ToolSemantics::Write) {
        return true;
    }

    // Network to non-public domains ‚Üí confirm
    if !config.allowed_domains.is_empty()
        && !config.allowed_domains.iter().all(|d| is_public_api(d))
    {
        return true;
    }

    false
}
```

Approval message:

```
I've created a skill "uptime-checker" with 2 tools:
  ‚Ä¢ check_uptime ‚Äî HTTP status + response time
  ‚Ä¢ check_ssl_expiry ‚Äî SSL cert expiry date

Permissions requested:
  üåê Network: status.myapp.io, api.myapp.io
  üîë Credentials: none
  üìù Write operations: none

Deploy this skill? (yes/no)
```

---

## 8. Deployment

Deployment is identical to the Dynamic Integrations spawn path:

```rust
FlowState::Deploying => {
    // Reuse the EXACT same spawn logic as third-party MCP servers
    let mcp_config = McpServerConfig {
        name: skill_config.name.clone(),
        command: skill_config.server.command.clone(),
        args: skill_config.server.args.clone(),
        working_dir: Some(skill_dir.to_path_buf()),
        allowed_domains: skill_config.allowed_domains.clone(),
        auth: skill_config.auth.clone(),  // vault refs
        label: skill_config.label.clone(),
        sandbox: skill_config.sandbox.clone(),
        source: ToolSource::Generated,  // vs ToolSource::ThirdParty
    };

    let tools = self.mcp_manager.spawn_server(mcp_config).await?;
    let tool_count = tools.len();

    // Register in skill index for semantic retrieval
    self.skill_index.add(&skill_config, &tools).await?;

    self.gateway.send(principal, &format!(
        "‚úì {} ready. {} tools available.",
        skill_config.name, tool_count
    )).await;

    flow.state = SkillForgeState::Complete;
    self.active_flows.remove(principal);
    Ok(())
}
```

After deployment, the skill is indistinguishable from a third-party MCP server in the pipeline. Phase 1 (Plan) selects it, Phase 2 (Execute) calls it, Phase 3 (Synthesize) formats the output.

---

## 9. Skill Retrieval: How the Planner Finds Skills

With 5 built-in tools + N third-party MCP tools + M self-created skills, the planner can't see everything. The research shows accuracy degrades past ~10 tools. PFAR uses two-stage retrieval:

### Stage 1: Semantic pre-filter (kernel, no LLM)

At Phase 1 entry, before the Planner LLM call, the kernel runs embedding similarity search over all registered tools.

```rust
pub struct SkillIndex {
    embeddings: HashMap<String, Vec<f32>>,  // tool_id ‚Üí embedding
    metadata: HashMap<String, ToolMetadata>,
}

impl SkillIndex {
    /// Returns top-k tools relevant to the user's message
    fn retrieve(&self, query_embedding: &[f32], k: usize) -> Vec<ToolMetadata> {
        let mut scored: Vec<_> = self.embeddings.iter()
            .map(|(id, emb)| {
                let score = cosine_similarity(query_embedding, emb);
                (id, score)
            })
            .collect();
        scored.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        scored.into_iter()
            .take(k)
            .filter_map(|(id, _)| self.metadata.get(id).cloned())
            .collect()
    }
}
```

Embeddings are computed from the concatenation of: tool name + description + keywords + example_queries. A local embedding model (e5-small, ~33MB, runs on CPU in ~5ms) generates vectors at skill registration time.

### Stage 2: Planner selects from candidates

The Planner's system prompt includes only the top-k tools (k=10 default) plus always-included built-in tools. Each tool entry is ~100 tokens (name + description + parameter schema).

```
Available tools for this request:

1. uptime-checker.check_uptime
   Check if an HTTP endpoint is up. Returns status code, response time.
   Parameters: url (string, required)

2. uptime-checker.check_ssl_expiry
   Check SSL certificate expiry date for a hostname.
   Parameters: hostname (string, required), port (int, default 443)

3. notion.search
   Search Notion workspace for pages and databases.
   Parameters: query (string, required)

... (up to 10 tools)
```

This keeps the Planner in the 5-10 tool sweet spot regardless of how many skills exist.

### Search entries come from skill.toml

```toml
[search]
keywords = ["uptime", "status", "health check", "ping", "monitoring"]
example_queries = [
    "is my server up?",
    "check uptime for status.myapp.io",
]
```

The `example_queries` field is critical ‚Äî it provides the embedding model with natural-language phrasings of what the tool does, dramatically improving retrieval for colloquial requests ("is my site down?" ‚Üí matches "check uptime").

---

## 10. Iterative Improvement

Skills improve through three mechanisms:

### 10a. Fix on failure

When a skill tool returns an error during normal use, Phase 3 (Synthesize) reports it. The owner can say "fix it":

```
Owner: "check uptime for status.myapp.io"
PFAR: "The uptime check failed: ConnectionError ‚Äî SSL handshake timeout after 5s"
Owner: "fix the timeout, make it 15 seconds"
‚Üí SkillForge activates in edit mode:
  - Loads existing server.py
  - LLM modifies the timeout
  - Re-tests in sandbox
  - Hot-reloads (stop old MCP server, start new one)
  - "‚úì uptime-checker updated. Timeout changed to 15s."
```

### 10b. Enhance on request

Owner asks for new capabilities on an existing skill:

```
Owner: "add response body size to the uptime check"
‚Üí SkillForge loads existing skill
‚Üí LLM adds `content_length` field to check_uptime return value
‚Üí Re-test ‚Üí hot-reload ‚Üí done
```

### 10c. Auto-improve from usage patterns (Phase 5+)

Track tool call success/failure rates. If a skill fails >30% of calls:
1. Collect recent error logs
2. Feed to LLM with original code
3. Generate improved version
4. Test ‚Üí deploy with owner notification

This is LATM's "tool refinement" pattern applied to production.

### Hot-reload

```rust
impl McpServerManager {
    async fn hot_reload_skill(&self, skill_name: &str) -> Result<()> {
        // 1. Stop existing MCP server
        self.stop_server(skill_name).await?;

        // 2. Deregister old tools
        self.tool_registry.deregister_server(skill_name);

        // 3. Re-read updated skill.toml and server.py
        let config = load_skill_config(skill_name)?;

        // 4. Spawn new server (same sandbox path)
        let tools = self.spawn_server(config).await?;

        // 5. Re-register tools + update embeddings
        self.skill_index.update(skill_name, &tools).await?;

        Ok(())
    }
}
```

---

## 11. Security Properties

Self-created skills get the **exact same security enforcement** as third-party MCP servers. No exceptions.

| Property | Enforcement | Same as third-party? |
|---|---|---|
| Network isolation | Bubblewrap `--unshare-net` + domain proxy | ‚úÖ Identical |
| Domain allowlist | Proxy blocks non-declared domains | ‚úÖ Identical |
| Filesystem isolation | Read-only mounts, /tmp only, no vault access | ‚úÖ Identical |
| Credential scoping | Vault refs resolved at spawn, env vars only | ‚úÖ Identical |
| Label enforcement | Output labeled per skill.toml config | ‚úÖ Identical |
| Write approval | Destructive tools trigger taint/approval | ‚úÖ Identical |
| Process isolation | PID namespace, die-with-parent | ‚úÖ Identical |
| Audit | All tool calls logged | ‚úÖ Identical |
| **Code review** | **LLM-generated, tested in sandbox** | ‚ö†Ô∏è Different ‚Äî no human review |
| **Approval gate** | **Risk-tiered: auto/confirm/explicit** | ‚ö†Ô∏è Different ‚Äî third-party always manual |

### What a malicious/buggy skill CAN'T do

Even if the LLM generates malicious code (prompt injection in API response ‚Üí skill creation):

- **Can't reach undeclared domains**: proxy blocks it
- **Can't read vault secrets**: no filesystem access to `~/.pfar/vault/`
- **Can't read other skills' data**: PID namespace, separate sandboxes
- **Can't survive restart without re-approval**: skills persist as files, but re-spawn goes through same validation
- **Can't escalate privileges**: bubblewrap drops capabilities
- **Can't modify PFAR's own code**: read-only filesystem mounts

### What a malicious/buggy skill CAN do

- Exfiltrate data to its declared allowed_domains (same risk as third-party MCP servers)
- Consume CPU/memory up to sandbox limits
- Return incorrect/misleading data
- Crash (handled by MCP server lifecycle management)

These are the **same residual risks** as any MCP server. Self-created skills are not more dangerous than third-party ones ‚Äî arguably less, since the LLM generates code for a specific purpose rather than importing an opaque npm package.

---

## 12. Skill Types and Examples

### Type 1: Utility scripts (no network)

```
Owner: "create a tool to convert between timezones"
‚Üí Pure Python, no allowed_domains, auto-approved
‚Üí Tools: convert_timezone, list_timezones
```

### Type 2: API wrappers (network read)

```
Owner: "I want to check weather from OpenWeatherMap, here's my API key: owm_abc123"
‚Üí allowed_domains: ["api.openweathermap.org"]
‚Üí Credential: vault:openweathermap_key ‚Üí OWM_API_KEY env var
‚Üí Approval gate: credentials needed
‚Üí Tools: get_weather, get_forecast
```

### Type 3: Service integration (no existing MCP server)

```
Owner: "connect to my Hetzner Cloud, I want to list and reboot servers"
‚Üí allowed_domains: ["api.hetzner.cloud"]
‚Üí Credential: vault:hetzner_token ‚Üí HETZNER_API_TOKEN env var
‚Üí Approval gate: write operations (reboot)
‚Üí Tools: list_servers, get_server_status, reboot_server
```

### Type 4: Data pipeline (multi-step)

```
Owner: "create a tool that fetches my bank transactions CSV from email,
        categorizes spending, and gives me a monthly summary"
‚Üí Depends on: existing email tool (for fetch) + new CSV parser skill
‚Üí LLM generates a skill that calls other tools via internal composition
‚Üí Tools: categorize_transactions, monthly_summary
```

### Type 5: Scheduled automation (with cron)

```
Owner: "every morning at 8am, check uptime for all my servers and
        send me a summary"
‚Üí Uses existing uptime-checker skill + cron scheduler
‚Üí SkillForge creates a workflow definition, not a new MCP server
‚Üí Cron triggers: call uptime tools ‚Üí synthesize report ‚Üí send to owner
```

---

## 13. Integration with main.rs

```rust
loop {
    let msg = gateway.recv().await?;

    // 1. Active flow intercept (credential paste, cancel, approval response)
    if flow_manager.intercept(&msg).await {
        continue;
    }

    // 2. Flow-starting commands
    if let Some(service) = parse_connect_command(&msg.text) {
        flow_manager.start_integration_setup(&service, &msg.principal_id).await?;
        continue;
    }
    if let Some(request) = parse_skill_request(&msg.text) {
        flow_manager.start_skill_creation(&request, &msg.principal_id).await?;
        continue;
    }

    // 3. Normal pipeline with semantic tool retrieval
    let extraction = phase0_extract(&msg).await?;

    if should_use_full_pipeline(&extraction) {
        // Semantic pre-filter: find relevant tools for this query
        let query_embedding = embed(&msg.text).await?;
        let candidate_tools = skill_index.retrieve(&query_embedding, 10);
        let builtin_tools = tool_registry.get_builtins();
        let available_tools = merge_tools(builtin_tools, candidate_tools);

        let plan = phase1_plan(&extraction, &available_tools).await?;
        let results = phase2_execute(&plan).await?;
        let response = phase3_synthesize(&results).await?;

        // Check if planner suggested skill creation
        if plan.suggestion == "no_matching_tool" {
            // Append suggestion to response
            response.append("\n\nI don't have a tool for this yet. Want me to create one?");
        }

        gateway.send(&msg.principal_id, &response).await;
    } else {
        let response = fast_path_synthesize(&msg, &extraction).await?;
        gateway.send(&msg.principal_id, &response).await;
    }
}
```

---

## 14. Skill Persistence and Startup

### On PFAR startup

```rust
async fn load_persisted_skills(mcp_manager: &McpServerManager, skill_index: &SkillIndex) {
    // 1. Load third-party integrations
    for config in read_dir("~/.pfar/mcp/") {
        if vault_has_credentials(&config) {
            mcp_manager.spawn_server(config).await;
        }
    }

    // 2. Load self-created skills
    for skill_dir in read_dir("~/.pfar/skills/") {
        let config = load_skill_config(&skill_dir)?;
        mcp_manager.spawn_server(config).await;
        skill_index.add(&config).await;
    }
}
```

Skills survive restarts. They persist as files on disk and re-spawn on startup.

### Skill management commands

Normal pipeline handles these (not KernelFlow):

```
"list my skills"        ‚Üí show all self-created skills with status
"remove uptime-checker" ‚Üí stop MCP server, delete skill directory
"disable weather"       ‚Üí stop MCP server, keep files
"edit uptime-checker"   ‚Üí enter SkillForge edit mode
"show uptime-checker code" ‚Üí display server.py (read-only, for owner review)
```

---

## 15. The Full Trace

**"I need to check if my servers are up"**

1. `parse_skill_request()` ‚Üí None (not explicit)
2. Normal pipeline ‚Üí Phase 0: Extract intent "check server uptime"
3. Phase 1: `skill_index.retrieve("check server uptime", 10)` ‚Üí no matches
4. Phase 1: Planner returns `suggestion: "no_matching_tool"`
5. Phase 3: "I don't have a server monitoring tool yet. Want me to create one? I'd build an HTTP uptime checker with response time measurement."
6. Owner: "yeah, and add SSL cert check too"

**Skill creation begins**

7. `parse_skill_request("yeah, and add SSL cert check too")` ‚Üí flows match context ‚Üí SkillForge activates
8. SkillForge: Designing ‚Üí "Which URLs should I be able to reach? Just for the domain allowlist."
9. Owner: "status.myapp.io and api.myapp.io"
10. SkillForge: requirements complete ‚Üí Generating
11. LLM generates server.py + skill.toml + test_server.py
12. SkillForge: Testing ‚Üí spawn ephemeral sandbox ‚Üí run pytest ‚Üí pass
13. SkillForge: AwaitingApproval ‚Üí "Skill wants network access to status.myapp.io, api.myapp.io. No credentials needed. Deploy?"
14. Owner: "yes"
15. SkillForge: Deploying ‚Üí spawn bubblewrap ‚Üí MCP handshake ‚Üí 2 tools discovered
16. SkillForge: Complete ‚Üí "‚úì uptime-checker ready. Tools: check_uptime, check_ssl_expiry"

**Using the skill**

17. Owner: "check if status.myapp.io is up"
18. Phase 0: Extract intent "check uptime"
19. Phase 1: `skill_index.retrieve("check uptime status.myapp.io", 10)` ‚Üí uptime-checker.check_uptime (0.94 similarity)
20. Phase 1: Plan `[{tool: "uptime-checker.check_uptime", args: {url: "https://status.myapp.io"}}]`
21. Phase 2: Kernel ‚Üí MCP call ‚Üí sandbox ‚Üí httpx ‚Üí proxy ‚Üí status.myapp.io ‚Üí response
22. Phase 3: "status.myapp.io is up. Status 200, response time 142ms."

**Improving the skill**

23. Owner: "the timeout is too short, make it 30 seconds and add response body size"
24. SkillForge edit mode: loads server.py ‚Üí LLM modifies ‚Üí re-test ‚Üí hot-reload
25. "‚úì uptime-checker updated. Timeout 30s, body size now included."

---

## 16. What This Enables

With Dynamic Integrations (Phase 3) + Self-Extending Skills (Phase 4), PFAR can:

- **Connect known services** conversationally (Notion, GitHub, Slack via pre-built MCP servers)
- **Create custom tools** for any HTTP API, data processing, or computation
- **Improve tools** iteratively based on usage and feedback
- **Accumulate capabilities** across sessions ‚Äî skills persist and are retrieved semantically
- **Maintain security** throughout ‚Äî every generated skill runs in the same sandbox as third-party code

The agent gets better the more you use it. Not through fine-tuning or prompt engineering, but by building a library of executable capabilities ‚Äî each sandboxed, scoped, and retrievable.

---

## 17. Implementation Checklist

### SkillForge KernelFlow
- [ ] `SkillForgeState` enum with all states
- [ ] `start_skill_creation()` ‚Äî enter Designing state
- [ ] `advance()` ‚Äî state machine driver
- [ ] `parse_skill_request()` ‚Äî detect creation intents
- [ ] LLM prompt template for code generation
- [ ] Error feedback loop (test fail ‚Üí regenerate)
- [ ] Edit mode for existing skills

### Code generation
- [ ] FastMCP template with PFAR constraints
- [ ] Few-shot examples (3-5 reference skills)
- [ ] skill.toml generation with search metadata
- [ ] test_server.py generation
- [ ] Pre-installed Python packages list

### Testing sandbox
- [ ] Ephemeral bubblewrap sandbox for test runs
- [ ] pytest execution inside sandbox
- [ ] MCP handshake verification
- [ ] 30-second hard timeout
- [ ] Error capture and formatting for LLM feedback

### Approval gates
- [ ] Risk classification (auto/confirm/explicit)
- [ ] Approval message formatting
- [ ] Owner response handling (yes/no/modify)

### Deployment (reuse from Dynamic Integrations)
- [ ] Spawn in bubblewrap (same path as third-party)
- [ ] MCP tool discovery
- [ ] Tool registration in pipeline
- [ ] Vault resolution for credential env vars

### Skill retrieval
- [ ] Local embedding model (e5-small or similar)
- [ ] Embedding computation at registration
- [ ] Cosine similarity search
- [ ] Top-k retrieval integration with Phase 1
- [ ] Progressive disclosure (metadata first, full schema on demand)

### Skill persistence
- [ ] Write skill.toml + server.py + tests to `~/.pfar/skills/`
- [ ] Load and spawn on startup
- [ ] Skill management commands (list, remove, disable, edit)

### Iterative improvement
- [ ] Edit mode: load existing ‚Üí modify ‚Üí re-test ‚Üí hot-reload
- [ ] Hot-reload: stop server ‚Üí update files ‚Üí re-spawn
- [ ] Re-index embeddings after update

### Tests
- [ ] Full flow: "create tool" ‚Üí generate ‚Üí test ‚Üí approve ‚Üí deploy ‚Üí use
- [ ] Test failure ‚Üí regenerate ‚Üí succeed on retry
- [ ] Pure computation skill auto-approved
- [ ] Network skill requires domain confirmation
- [ ] Credential skill requires explicit approval
- [ ] Skill survives PFAR restart
- [ ] Semantic retrieval finds skill for natural queries
- [ ] Skill edit ‚Üí hot-reload ‚Üí updated behavior
- [ ] Malicious code can't escape sandbox
- [ ] Malicious code can't reach undeclared domains
- [ ] 3 generation failures ‚Üí graceful failure message
